import os
import time
import fitz  # PyMuPDF –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PDF
import yaml
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import ScrapeWebsiteTool, SerperDevTool
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

# ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# ‚úÖ –ò–º–ø–æ—Ä—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
scrap_web_tool = ScrapeWebsiteTool()
search_tool = SerperDevTool()

from tools.custom_tool import clean_text_tool  # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
# from special_resume.tools.custom_tool import clean_text_tool


@CrewBase
class SpecialResume:
    """Crew for resume optimization and interview preparation"""

    agents_config = 'config/agents.yaml'
    tasks_config = 'config/tasks.yaml'

    def __init__(self):
        # self.llm = ChatOpenAI(
        #     model="openrouter/google/gemini-flash-1.5",
        #     openai_api_base="https://openrouter.ai/api/v1",
        #     openai_api_key=os.getenv("OPENROUTER_API_KEY"),
        # )
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",  
            openai_api_key=os.getenv("OPENAI_API_KEY")  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º OpenAI –∫–ª—é—á
            )

    @agent
    def researcher(self) -> Agent:
        return Agent(
            config=self.agents_config["researcher"],
            verbose=True,
            tools=[search_tool, scrap_web_tool, clean_text_tool],  # ‚úÖ –ê–≥–µ–Ω—Ç —Ç–µ–ø–µ—Ä—å –º–æ–∂–µ—Ç –æ—á–∏—â–∞—Ç—å —Ç–µ–∫—Å—Ç —Å–∞–º!
            llm=self.llm
        )

    @agent
    def profiler(self) -> Agent:
        return Agent(
            config=self.agents_config["profiler"],
            verbose=True,
            llm=self.llm
        )

    @agent
    def resume_strategist(self) -> Agent:
        return Agent(
            config=self.agents_config["resume_strategist"],
            verbose=True,
            llm=self.llm
        )

    @agent
    def interview_preparer(self) -> Agent:
        return Agent(
            config=self.agents_config["interview_preparer"],
            verbose=True,
            llm=self.llm
        )

    @task
    def analyze_job_posting(self) -> Task:
        return Task(
            config=self.tasks_config["analyze_job_posting"]
        )

    @task
    def analyze_resume(self) -> Task:
        return Task(
            config=self.tasks_config["analyze_resume"]
        )

    @task
    def optimize_resume(self) -> Task:
        return Task(
            config=self.tasks_config["optimize_resume"],
            output_file="/Users/sergey/Desktop/CrewAI_resume/special_resume/outputsoutputs/trained_crew/tailored_resume.md"
        )

    @task
    def prepare_technical_questions(self) -> Task:
        return Task(
            config=self.tasks_config["prepare_technical_questions"],
            output_file="/Users/sergey/Desktop/CrewAI_resume/special_resume/outputsoutputs/trained_crew/interview_materials.md"
        )

    @crew
    def crew(self) -> Crew:
        return Crew(
            agents=self.agents,
            tasks=self.tasks,
            process=Process.sequential,
            verbose=True
        )

# ‚úÖ **–§—É–Ω–∫—Ü–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF**
def extract_text_from_pdf(pdf_path):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF –∏ –æ—á–∏—â–∞–µ—Ç –µ–≥–æ."""
    doc = fitz.open(pdf_path)
    text = "\n".join(page.get_text("text") for page in doc)
    return clean_text_tool.run(text)   # ‚úÖ –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º `clean_text_tool`

# ‚úÖ **–§—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ CrewAI**
def run_application_process(job_posting_url, resume_path="resume.pdf"):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç CrewAI, –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ —Ä–µ–∑—é–º–µ –ø–µ—Ä–µ–¥ –ø–æ–¥–∞—á–µ–π –∞–≥–µ–Ω—Ç–∞–º."""
    
    print("üöÄ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤–∞–∫–∞–Ω—Å–∏–∏...")
    scraped_text = scrap_web_tool.run(website_url=job_posting_url)
    job_description = clean_text_tool.run(scraped_text)  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º `clean_text_tool`

    print("üöÄ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—é–º–µ...")
    resume_text = extract_text_from_pdf(resume_path)

    print("üîó Job Posting URL:", job_posting_url)
    print("üìÑ Cleaned Job Description (first 500 chars):", job_description[:500])
    print("üìÑ Resume Text (first 500 chars):", resume_text[:500])

    # ‚úÖ **–ü–µ—Ä–µ–¥–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∞–≥–µ–Ω—Ç–∞–º!**
    result = SpecialResume().crew().kickoff(inputs={
        "job_description": job_description,  # ‚úÖ –ü–µ—Ä–µ–¥–∞—ë–º —É–∂–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç!
        "resume_text": resume_text
    })

    print("üìù CrewAI –∑–∞–≤–µ—Ä—à–∏–ª –æ–±—Ä–∞–±–æ—Ç–∫—É. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞...")

    time.sleep(2)

    tailored_resume_path = "/Users/sergey/Desktop/CrewAI_resume/special_resume/outputsoutputs/trained_crew/tailored_resume.md"
    if os.path.exists(tailored_resume_path):
        print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {tailored_resume_path}")
    else:
        print("‚ö†Ô∏è –ò—Ç–æ–≥–æ–≤–æ–µ —Ä–µ–∑—é–º–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ CrewAI.")

# ‚úÖ **–ó–∞–ø—É—Å–∫**
if __name__ == "__main__":
    run_application_process(
        "https://ods.ai/jobs/366c0d8e-fdb3-4e4b-9fd8-183d855a5962",
        resume_path="/Users/sergey/Desktop/CrewAI_resume/K–∞—Ä–ø–µ–Ω–∫–æ –°–µ—Ä–≥–µ–π.pdf"
    )